{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7541646,"sourceType":"datasetVersion","datasetId":4391692}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms, datasets\nimport torchvision.utils as vutils\nimport matplotlib.pyplot as plt\nimport os\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Parameters\nIMAGE_SIZE = 64\nCHANNELS = 1\nBATCH_SIZE = 256\nZ_DIM = 100\nEPOCHS = 300\nLEARNING_RATE = 0.0002\nNOISE_PARAM = 0.1\n\n# Prepare the data\ntransform = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,), (0.5,))\n])\n\nfrom PIL import Image\n\nclass CustomDataset(Dataset):\n    def __init__(self, root_dir, transform):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.images = [os.path.join(root_dir, img) for img in os.listdir(root_dir) if img.endswith(\".png\")]\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        img_path = self.images[idx]\n        img = Image.open(img_path).convert('L')  # Convert to grayscale\n        img = self.transform(img)\n        return img\n\n\ndataset = CustomDataset(root_dir=\"/kaggle/input/gan-dataset/waqar_pics\", transform=transform)\ndataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n\n# Define the generator\nclass Generator(nn.Module):\n    def __init__(self, z_dim, channels):\n        super(Generator, self).__init__()\n        self.model = nn.Sequential(\n            nn.ConvTranspose2d(z_dim, 512, kernel_size=4, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.ConvTranspose2d(64, channels, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        return self.model(z.view(z.size(0), z.size(1), 1, 1))\n\n# Define the discriminator\nclass Discriminator(nn.Module):\n    def __init__(self, channels):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(channels, 64, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.3),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.3),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.3),\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Dropout(0.3),\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0, bias=False),\n            nn.Flatten(),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n# Initialize generator and discriminator\ngenerator = Generator(Z_DIM, CHANNELS).to(device)\ndiscriminator = Discriminator(CHANNELS).to(device)\n\n# Loss function and optimizers\ncriterion = nn.BCELoss()\nd_optimizer = optim.Adam(discriminator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\ng_optimizer = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n\n# Training loop\nfor epoch in range(EPOCHS):\n    for real_images in dataloader:\n        real_images = real_images.to(device)\n        batch_size = real_images.size(0)\n\n        # Train discriminator\n        d_optimizer.zero_grad()\n        real_labels = torch.ones(batch_size, 1).to(device)\n        real_outputs = discriminator(real_images)\n        real_loss = criterion(real_outputs, real_labels)\n        real_loss.backward()\n\n        z = torch.randn(batch_size, Z_DIM, 1, 1).to(device)\n        fake_images = generator(z)\n        fake_labels = torch.zeros(batch_size, 1).to(device)\n        fake_outputs = discriminator(fake_images.detach())\n        fake_loss = criterion(fake_outputs, fake_labels)\n        fake_loss.backward()\n\n        d_optimizer.step()\n\n        # Train generator\n        g_optimizer.zero_grad()\n        outputs = discriminator(fake_images)\n        g_loss = criterion(outputs, real_labels)\n        g_loss.backward()\n        g_optimizer.step()\n\n    # Print progress\n    if epoch % 10 == 0:\n        print(f\"Epoch [{epoch}/{EPOCHS}], d_loss: {real_loss.item() + fake_loss.item()}, g_loss: {g_loss.item()}\")\n\n    # Save generated images\n    if epoch % 50 == 0:\n        folder_name = f\"{epoch}_epochs_pics\"\n        os.makedirs(folder_name, exist_ok=True)\n\n        with torch.no_grad():\n            fake_images = generator(torch.randn(25, Z_DIM, 1, 1).to(device)).detach().cpu()\n\n        for i in range(fake_images.size(0)):\n            vutils.save_image(fake_images[i], f\"{folder_name}/generated_image_epoch_{epoch}_{i}.png\", normalize=True)\n\n\n# Save models\ntorch.save(generator.state_dict(), \"generator.pth\")\ntorch.save(discriminator.state_dict(), \"discriminator.pth\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-02T20:58:31.887975Z","iopub.execute_input":"2024-02-02T20:58:31.888331Z","iopub.status.idle":"2024-02-02T21:50:26.221047Z","shell.execute_reply.started":"2024-02-02T20:58:31.888301Z","shell.execute_reply":"2024-02-02T21:50:26.219822Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Epoch [0/300], d_loss: 0.3186000809073448, g_loss: 6.252570152282715\nEpoch [10/300], d_loss: 0.8392009735107422, g_loss: 2.5531067848205566\nEpoch [20/300], d_loss: 0.8546193018555641, g_loss: 1.1791412830352783\nEpoch [30/300], d_loss: 0.5591198392212391, g_loss: 3.754275321960449\nEpoch [40/300], d_loss: 0.3303839787840843, g_loss: 3.7111949920654297\nEpoch [50/300], d_loss: 0.32125783897936344, g_loss: 2.2335636615753174\nEpoch [60/300], d_loss: 0.1438494697213173, g_loss: 3.759979248046875\nEpoch [70/300], d_loss: 0.19290368258953094, g_loss: 4.202148914337158\nEpoch [80/300], d_loss: 0.15114159882068634, g_loss: 4.324741363525391\nEpoch [90/300], d_loss: 0.1481534205377102, g_loss: 4.21136999130249\nEpoch [100/300], d_loss: 0.14825372397899628, g_loss: 3.4284093379974365\nEpoch [110/300], d_loss: 0.09734949469566345, g_loss: 4.145509719848633\nEpoch [120/300], d_loss: 0.2027099598199129, g_loss: 3.428286552429199\nEpoch [130/300], d_loss: 0.16073133051395416, g_loss: 3.5599851608276367\nEpoch [140/300], d_loss: 0.15106293186545372, g_loss: 4.642288684844971\nEpoch [150/300], d_loss: 0.08502279035747051, g_loss: 5.2110466957092285\nEpoch [160/300], d_loss: 0.09052848629653454, g_loss: 4.997434616088867\nEpoch [170/300], d_loss: 0.11349986493587494, g_loss: 4.674740791320801\nEpoch [180/300], d_loss: 0.10580218210816383, g_loss: 4.864233016967773\nEpoch [190/300], d_loss: 0.11719057708978653, g_loss: 5.595864295959473\nEpoch [200/300], d_loss: 0.07263552397489548, g_loss: 4.9261064529418945\nEpoch [210/300], d_loss: 2.287005024496466, g_loss: 1.0796538591384888\nEpoch [220/300], d_loss: 0.06629223376512527, g_loss: 5.326775550842285\nEpoch [230/300], d_loss: 0.053570314310491085, g_loss: 5.109992504119873\nEpoch [240/300], d_loss: 0.07737322337925434, g_loss: 5.683647155761719\nEpoch [250/300], d_loss: 0.053260937333106995, g_loss: 5.676505088806152\nEpoch [260/300], d_loss: 0.08774608001112938, g_loss: 5.386712074279785\nEpoch [270/300], d_loss: 0.06040324456989765, g_loss: 5.865034103393555\nEpoch [280/300], d_loss: 0.03543213568627834, g_loss: 5.739813804626465\nEpoch [290/300], d_loss: 0.06092074140906334, g_loss: 5.496167182922363\n","output_type":"stream"}]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Zip and download folders\nfor epoch in range(0, EPOCHS + 1, 50):\n    folder_name = f\"{epoch}_epochs_pics\"\n    shutil.make_archive(folder_name, 'zip', folder_name)\n\n# Move zip files to /kaggle/working directory\nfor epoch in range(0, EPOCHS + 1, 50):\n    folder_name = f\"{epoch}_epochs_pics\"\n    shutil.move(f\"{folder_name}.zip\", f\"/kaggle/working/{folder_name}.zip\")\n\n# Create a zip file containing all the zip files\nshutil.make_archive(\"all_epochs_pics\", 'zip', \"/kaggle/working\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}